# Boticelli Configuration
#
# This file defines rate limits and pricing for LLM API providers.
# Rate limits change over time - update these values as providers adjust their quotas.
#
# Configuration precedence (highest to lowest):
# 1. CLI flags (--tier, --rpm, --tpm, etc.)
# 2. Environment variables (GEMINI_TIER, ANTHROPIC_TIER, etc.)
# 3. User override: ./boticelli.toml or ~/.config/boticelli/boticelli.toml
# 4. Default: boticelli.toml (this file)

# ============================================================================
# Context Path Configuration
# ============================================================================
# Base directory for resolving file references in narrative TOML files.
# When a narrative references a file (e.g., file = "BOTTICELLI_CONTEXT.md"),
# the system searches recursively from this path.
#
# Default: Current workspace root (for development)
# Override: Set to your project directory or a shared context repository

[context]
# Search base directory (defaults to workspace root if not set)
# path = "/path/to/your/context/files"

# ============================================================================
# Default Budget Multipliers
# ============================================================================
# Apply conservative multipliers to all rate limits by default to avoid
# hitting quota edges. Set at 80% to provide safety margin.
#
# Override in narrative TOML or via CLI flags:
# - Narrative: [narrative.budget] or [narratives.name.carousel.budget]
# - CLI: --rpm-multiplier 0.9 --rpd-multiplier 0.5

[budget]
rpm_multiplier = 0.8  # Use 80% of requests per minute
tpm_multiplier = 0.8  # Use 80% of tokens per minute  
rpd_multiplier = 0.8  # Use 80% of requests per day

# ============================================================================
# Gemini (Google AI)
# ============================================================================
# Last updated: 2025-01-15
# Source: https://ai.google.dev/gemini-api/docs/rate-limits
#
# Note: Different Gemini models have different rate limits even within the
# same tier. Model-specific overrides are specified below tier defaults.

[providers.gemini]
default_tier = "free"

# Tier-level defaults (used when no model-specific override exists)
[providers.gemini.tiers.free]
name = "Free"
rpm = 10                             # Default (gemini-2.5-flash)
tpm = 250_000                        # Default (gemini-2.5-flash)
rpd = 250                            # Default (gemini-2.5-flash)
max_concurrent = 1
cost_per_million_input_tokens = 0.0
cost_per_million_output_tokens = 0.0

# Model-specific overrides for free tier
[providers.gemini.tiers.free.models."gemini-2.5-pro"]
rpm = 2
tpm = 125_000
rpd = 50

[providers.gemini.tiers.free.models."gemini-2.5-flash"]
rpm = 10
tpm = 250_000
rpd = 250

[providers.gemini.tiers.free.models."gemini-2.5-flash-lite"]
rpm = 15
tpm = 250_000
rpd = 1_000

[providers.gemini.tiers.free.models."gemini-2.0-flash"]
rpm = 15
tpm = 1_000_000
rpd = 200

# Pay-as-you-go tier defaults
[providers.gemini.tiers.payasyougo]
name = "Pay-as-you-go"
rpm = 1_000            # Default (gemini-2.5-flash)
tpm = 1_000_000        # Default (gemini-2.5-flash)
# rpd not specified = unlimited
max_concurrent = 1
cost_per_million_input_tokens = 0.075 # Gemini 2.0 Flash pricing
cost_per_million_output_tokens = 0.30

# Model-specific overrides for pay-as-you-go tier
[providers.gemini.tiers.payasyougo.models."gemini-2.5-pro"]
rpm = 150
tpm = 2_000_000
rpd = 10_000

[providers.gemini.tiers.payasyougo.models."gemini-2.5-flash"]
rpm = 1_000
tpm = 1_000_000
rpd = 10_000

[providers.gemini.tiers.payasyougo.models."gemini-2.5-flash-lite"]
rpm = 4_000
tpm = 4_000_000
# rpd unlimited

[providers.gemini.tiers.payasyougo.models."gemini-2.0-flash"]
rpm = 2_000
tpm = 4_000_000
# rpd unlimited

# ============================================================================
# Anthropic (Claude)
# ============================================================================
# Last updated: 2025-01-11
# Source: https://docs.anthropic.com/claude/docs/rate-limits

[providers.anthropic]
default_tier = "tier1"

[providers.anthropic.tiers.tier1]
name = "Tier 1"
rpm = 5
tpm = 20_000
max_concurrent = 5
cost_per_million_input_tokens = 3.0   # Claude 3.5 Sonnet
cost_per_million_output_tokens = 15.0

[providers.anthropic.tiers.tier2]
name = "Tier 2"
rpm = 50
tpm = 40_000
max_concurrent = 5
cost_per_million_input_tokens = 3.0
cost_per_million_output_tokens = 15.0

[providers.anthropic.tiers.tier3]
name = "Tier 3"
rpm = 1000
tpm = 80_000
max_concurrent = 5
cost_per_million_input_tokens = 3.0
cost_per_million_output_tokens = 15.0

[providers.anthropic.tiers.tier4]
name = "Tier 4"
rpm = 2000
tpm = 160_000
max_concurrent = 5
cost_per_million_input_tokens = 3.0
cost_per_million_output_tokens = 15.0

# ============================================================================
# OpenAI
# ============================================================================
# Last updated: 2025-01-11
# Source: https://platform.openai.com/docs/guides/rate-limits

[providers.openai]
default_tier = "tier1"

[providers.openai.tiers.free]
name = "Free"
rpm = 3
tpm = 40_000
rpd = 200
max_concurrent = 50
cost_per_million_input_tokens = 0.0
cost_per_million_output_tokens = 0.0

[providers.openai.tiers.tier1]
name = "Tier 1"
rpm = 500
tpm = 200_000
max_concurrent = 50
cost_per_million_input_tokens = 2.50  # GPT-4 Turbo (varies by model)
cost_per_million_output_tokens = 10.0

[providers.openai.tiers.tier2]
name = "Tier 2"
rpm = 5000
tpm = 2_000_000
max_concurrent = 50
cost_per_million_input_tokens = 2.50
cost_per_million_output_tokens = 10.0

[providers.openai.tiers.tier3]
name = "Tier 3"
rpm = 10000
tpm = 10_000_000
max_concurrent = 50
cost_per_million_input_tokens = 2.50
cost_per_million_output_tokens = 10.0

[providers.openai.tiers.tier4]
name = "Tier 4"
rpm = 10000
tpm = 30_000_000
max_concurrent = 50
cost_per_million_input_tokens = 2.50
cost_per_million_output_tokens = 10.0

[providers.openai.tiers.tier5]
name = "Tier 5"
rpm = 10000
tpm = 100_000_000
max_concurrent = 50
cost_per_million_input_tokens = 2.50
cost_per_million_output_tokens = 10.0

[inference]
backend = "local"
api_url = "http://localhost:8080"
model = "mistralai/Mistral-7B-Instruct-v0.2" # Must match the model you started

# ============================================================================
# Bot Server Configuration
# ============================================================================
# Configuration for automated content generation, curation, and posting bots.
# These bots run independently as background services orchestrated by the
# bot server.

[bot_server]
# Enable/disable the bot server
enabled = false

# Generation bot configuration
[bot_server.generation]
enabled = true
narrative = "discord/generation_carousel"  # Path relative to narratives/
narrative_name = "batch_generate"
interval_hours = 24  # Run once per day
model = "gemini-2.5-flash-lite"  # High RPD limit model

# Curation bot configuration
[bot_server.curation]
enabled = true
narrative = "discord/curation"
narrative_name = "curate"
check_interval_hours = 12  # Check for new content every 12 hours
batch_size = 10  # Process up to 10 items per batch
model = "gemini-2.5-flash"  # Balanced model

# Posting bot configuration
[bot_server.posting]
enabled = true
narrative = "discord/posting"
narrative_name = "post"
base_interval_hours = 6  # Base posting interval (every 6 hours)
jitter_percent = 30  # Â±30% randomization (4.2-7.8 hours)
channel_id = "1234567890"  # Discord channel ID for posts
model = "gemini-2.5-flash-lite"  # Lightweight model

# ============================================================================
# Additional Providers
# ============================================================================
# Add your custom providers or override defaults here.
# Example:
#
# [providers.mylocal]
# default_tier = "unlimited"
#
# [providers.mylocal.tiers.unlimited]
# name = "Local Model"
# # All limits omitted = no restrictions
